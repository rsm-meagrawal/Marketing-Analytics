[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nMegha Agrawal\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nMegha Agrawal\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nMegha Agrawal\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project4/hw2_questions.html",
    "href": "blog/project4/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n# Read in the data\ndata &lt;- read.csv(\"/Users/megha/Desktop/Marketing Analytics/mysite/blog/project4/blueprinty.csv\")\nhead(data)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\n\n\n\n\n\n\nShow Code\n# Load libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n# Transform iscustomer to a factor for clarity\ndata$iscustomer &lt;- factor(data$iscustomer, labels = c(\"Non-Customer\", \"Customer\"))\n\n# Plot histograms with pretty formatting\nggplot(data, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(binwidth = 1, alpha = 0.7, position = 'identity', color = \"black\") +\n  facet_wrap(~iscustomer) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n       x = \"Number of Patents\",\n       y = \"Frequency\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n\n\n\nThe histogram compares the distribution of the number of patents held by customers versus non-customers:\n\nNon-Customers: The patent count is predominantly concentrated around lower values (0-5 patents), indicating most non-customers possess fewer patents.\nCustomers: Patents held by customers show a slightly broader distribution, extending toward higher patent counts, with a higher mean overall compared to non-customers.\n\nThis suggests that customers generally tend to hold more patents than non-customers, implying a potential link between customer status and innovation or patent activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nShow Code\n# Convert customer status into factor for clear labeling\ndata$iscustomer &lt;- factor(data$iscustomer, labels = c(\"Non-Customer\", \"Customer\"))\n\n# Age Distribution Boxplot\nggplot(data, aes(x = iscustomer, y = age, fill = iscustomer)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Age Distribution by Customer Status\",\n       x = \"Customer Status\",\n       y = \"Age\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n\n\n\nShow Code\n# Region Distribution Barplot\ndata %&gt;%\n  group_by(region, iscustomer) %&gt;%\n  summarise(count = n()) %&gt;%\n  group_by(iscustomer) %&gt;%\n  mutate(percentage = count / sum(count) * 100) %&gt;%\n  ggplot(aes(x = region, y = percentage, fill = iscustomer)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(title = \"Region Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Percentage (%)\",\n       fill = \"Customer Status\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"orange\", \"tomato\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nAge Distribution: Both customer groups (customers and non-customers) show similar median ages. Customers have slightly less variation in age, with fewer extreme outliers, suggesting that age alone does not significantly differentiate customers from non-customers.\nRegion Distribution: There’s a clear regional difference between customers and non-customers: Customers are heavily concentrated in the Northeast region. Non-customers are more evenly distributed across regions, with a notable presence in the Southwest.\nThis suggests region might play a significant role in customer status, particularly with a strong customer base in the Northeast.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood. Let \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\). The probability mass function for each observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThen, the likelihood function for the entire sample is:\n\\[\n\\mathcal{L}(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nOr, more compactly:\n\\[\n\\mathcal{L}(\\lambda \\mid \\mathbf{Y}) = e^{-n\\lambda} \\lambda^{\\sum Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\n\n# Poisson likelihood function\npoisson_likelihood &lt;- function(lambda, Y) {\n  likelihood &lt;- prod(exp(-lambda) * lambda^Y / factorial(Y))\n  return(likelihood)\n}\n\n# Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  loglikelihood &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(loglikelihood)\n}\n\n\n\nShow Code\n# Observed patents\nY &lt;- data$patents\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\n# Range of lambda values\nlambda_values &lt;- seq(0.1, 10, length.out = 100)\n\n# Compute log-likelihood for each lambda\nloglik_values &lt;- sapply(lambda_values, poisson_loglikelihood, Y = Y)\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(lambda = lambda_values, loglikelihood = loglik_values)\n\n# Plot lambda vs. log-likelihood\nggplot(plot_data, aes(x = lambda, y = loglikelihood)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  labs(title = \"Poisson Log-Likelihood across Lambda Values\",\n       x = expression(lambda),\n       y = \"Log-Likelihood\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Write down the log-likelihood\nGiven ( Y_i () ):\n\\[\n\\ell(\\lambda|Y) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\n\nStep 2: Take the first derivative of the log-likelihood\nDifferentiate ((|Y)) with respect to ():\n\\[\n\\frac{d\\ell(\\lambda|Y)}{d\\lambda} = \\sum_{i=1}^{n}\\left(-1 + \\frac{Y_i}{\\lambda}\\right)\n= -n + \\frac{\\sum_{i=1}^{n}Y_i}{\\lambda}\n\\]\n\nStep 3: Set the derivative equal to zero and solve for ()\n\\[\n-n + \\frac{\\sum_{i=1}^{n}Y_i}{\\lambda} = 0\n\\]\nSolve for ():\n\\[\n\\frac{\\sum_{i=1}^{n}Y_i}{\\lambda} = n \\quad \\Rightarrow \\quad \\lambda_{MLE} = \\frac{\\sum_{i=1}^{n}Y_i}{n} = \\bar{Y}\n\\]\n\nConclusion:\nThe Maximum Likelihood Estimator (MLE) for () is the sample mean ({Y}), which intuitively matches our expectations from the Poisson distribution since ( E[Y] = ):\n\\[\n\\boxed{\\lambda_{MLE} = \\bar{Y}}\n\\]\n\n# Negative log-likelihood function\nneg_loglikelihood &lt;- function(lambda, Y) {\n  -sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\n# Find lambda MLE using optim()\nresult &lt;- optim(par = 1, fn = neg_loglikelihood, Y = Y, method = \"L-BFGS-B\", lower = 0.0001)\n\n# Extract MLE estimate\nlambda_mle &lt;- result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\\[\n\\ell(\\beta|Y,X) = \\sum_{i=1}^{n}\\left[-e^{X_i'\\beta} + Y_i(X_i'\\beta) - \\log(Y_i!)\\right]\n\\]\n\n\nShow Code\n# Construct X matrix with intercept, age, age squared, regions (dummy variables), and iscustomer\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = data)\n\n# Response variable\nY &lt;- data$patents\n\n# Negative log-likelihood function for Poisson regression\nneg_loglik_poisson_reg &lt;- function(beta, Y, X){\n  lambda &lt;- exp(X %*% beta)\n  -sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\n# Initial beta values\ninitial_beta &lt;- rep(0, ncol(X))\n\n# Find MLE using optim()\nresult &lt;- optim(initial_beta, \n                neg_loglik_poisson_reg, \n                Y = Y, \n                X = X,\n                method = \"BFGS\",\n                hessian = TRUE)\n\n# Extract estimates\nbeta_mle &lt;- result$par\n# Calculate standard errors from Hessian\nse_beta &lt;- sqrt(diag(solve(result$hessian)))\n\n# Output results table\nresults_table &lt;- data.frame(\n  Coefficient = beta_mle,\n  Std_Error = se_beta,\n  row.names = colnames(X)\n)\n\nprint(results_table)\n\n\n                    Coefficient    Std_Error\n(Intercept)        -0.125735915 0.1122180354\nage                 0.115793715 0.0063574230\nI(age^2)           -0.002228748 0.0000771291\nregionNortheast    -0.024556782 0.0433762879\nregionNorthwest    -0.034827790 0.0529311002\nregionSouth        -0.005441860 0.0524007440\nregionSouthwest    -0.037784109 0.0471722463\niscustomerCustomer  0.060665584 0.0320588299\n\n\n\n\nShow Code\n# Fit the Poisson regression model\npoisson_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, \n                     family = poisson(link = \"log\"), \n                     data = data)\n\n# Summarize the results\nsummary(poisson_model)\n\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = data)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -0.508920   0.183179  -2.778  0.00546 ** \nage                 0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)           -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast     0.029170   0.043625   0.669  0.50372    \nregionNorthwest    -0.017574   0.053781  -0.327  0.74383    \nregionSouth         0.056561   0.052662   1.074  0.28281    \nregionSouthwest     0.050576   0.047198   1.072  0.28391    \niscustomerCustomer  0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nIntercept: Baseline log count of patents when all covariates are zero.\nAge: Positive coefficient implies older firms tend to have higher patent counts.\nAge squared: Negative coefficient indicates diminishing returns with age—patent count increases initially and decreases for very old firms.\nRegion: Coefficients represent differences in patent counts relative to the reference region.\nCustomer Status (iscustomer): Positive and significant coefficient indicates customers of Blueprinty typically have more patents.\n\nUse exp(coef(poisson_model)) to clearly interpret coefficients as multiplicative changes in patent rates.\n\n\n\nThe average predicted increase in the number of patents attributable specifically to being a Blueprinty customer is approximately 0.79 patents per firm.\nThis indicates that firms using Blueprinty’s software can expect, on average, nearly one additional patent compared to non-customer firms.\nThis effect is both statistically significant (as previously established from regression results) and practically meaningful, clearly highlighting the positive impact of Blueprinty’s software on patent success."
  },
  {
    "objectID": "blog/project4/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project4/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\nData\n\n\n\n\n\n\n# Read in the data\ndata &lt;- read.csv(\"/Users/megha/Desktop/Marketing Analytics/mysite/blog/project4/blueprinty.csv\")\nhead(data)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\n\n\n\n\n\n\nShow Code\n# Load libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n# Transform iscustomer to a factor for clarity\ndata$iscustomer &lt;- factor(data$iscustomer, labels = c(\"Non-Customer\", \"Customer\"))\n\n# Plot histograms with pretty formatting\nggplot(data, aes(x = patents, fill = iscustomer)) +\n  geom_histogram(binwidth = 1, alpha = 0.7, position = 'identity', color = \"black\") +\n  facet_wrap(~iscustomer) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n       x = \"Number of Patents\",\n       y = \"Frequency\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n\n\n\nThe histogram compares the distribution of the number of patents held by customers versus non-customers:\n\nNon-Customers: The patent count is predominantly concentrated around lower values (0-5 patents), indicating most non-customers possess fewer patents.\nCustomers: Patents held by customers show a slightly broader distribution, extending toward higher patent counts, with a higher mean overall compared to non-customers.\n\nThis suggests that customers generally tend to hold more patents than non-customers, implying a potential link between customer status and innovation or patent activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nShow Code\n# Convert customer status into factor for clear labeling\ndata$iscustomer &lt;- factor(data$iscustomer, labels = c(\"Non-Customer\", \"Customer\"))\n\n# Age Distribution Boxplot\nggplot(data, aes(x = iscustomer, y = age, fill = iscustomer)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Age Distribution by Customer Status\",\n       x = \"Customer Status\",\n       y = \"Age\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n\n\n\nShow Code\n# Region Distribution Barplot\ndata %&gt;%\n  group_by(region, iscustomer) %&gt;%\n  summarise(count = n()) %&gt;%\n  group_by(iscustomer) %&gt;%\n  mutate(percentage = count / sum(count) * 100) %&gt;%\n  ggplot(aes(x = region, y = percentage, fill = iscustomer)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(title = \"Region Distribution by Customer Status\",\n       x = \"Region\",\n       y = \"Percentage (%)\",\n       fill = \"Customer Status\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"orange\", \"tomato\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nAge Distribution: Both customer groups (customers and non-customers) show similar median ages. Customers have slightly less variation in age, with fewer extreme outliers, suggesting that age alone does not significantly differentiate customers from non-customers.\nRegion Distribution: There’s a clear regional difference between customers and non-customers: Customers are heavily concentrated in the Northeast region. Non-customers are more evenly distributed across regions, with a notable presence in the Southwest.\nThis suggests region might play a significant role in customer status, particularly with a strong customer base in the Northeast.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood. Let \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\). The probability mass function for each observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThen, the likelihood function for the entire sample is:\n\\[\n\\mathcal{L}(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nOr, more compactly:\n\\[\n\\mathcal{L}(\\lambda \\mid \\mathbf{Y}) = e^{-n\\lambda} \\lambda^{\\sum Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\n\n# Poisson likelihood function\npoisson_likelihood &lt;- function(lambda, Y) {\n  likelihood &lt;- prod(exp(-lambda) * lambda^Y / factorial(Y))\n  return(likelihood)\n}\n\n# Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  loglikelihood &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(loglikelihood)\n}\n\n\n\nShow Code\n# Observed patents\nY &lt;- data$patents\n\n# Define the Poisson log-likelihood function\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\n# Range of lambda values\nlambda_values &lt;- seq(0.1, 10, length.out = 100)\n\n# Compute log-likelihood for each lambda\nloglik_values &lt;- sapply(lambda_values, poisson_loglikelihood, Y = Y)\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(lambda = lambda_values, loglikelihood = loglik_values)\n\n# Plot lambda vs. log-likelihood\nggplot(plot_data, aes(x = lambda, y = loglikelihood)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  labs(title = \"Poisson Log-Likelihood across Lambda Values\",\n       x = expression(lambda),\n       y = \"Log-Likelihood\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: Write down the log-likelihood\nGiven ( Y_i () ):\n\\[\n\\ell(\\lambda|Y) = \\sum_{i=1}^{n} \\left(-\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!)\\right)\n\\]\n\nStep 2: Take the first derivative of the log-likelihood\nDifferentiate ((|Y)) with respect to ():\n\\[\n\\frac{d\\ell(\\lambda|Y)}{d\\lambda} = \\sum_{i=1}^{n}\\left(-1 + \\frac{Y_i}{\\lambda}\\right)\n= -n + \\frac{\\sum_{i=1}^{n}Y_i}{\\lambda}\n\\]\n\nStep 3: Set the derivative equal to zero and solve for ()\n\\[\n-n + \\frac{\\sum_{i=1}^{n}Y_i}{\\lambda} = 0\n\\]\nSolve for ():\n\\[\n\\frac{\\sum_{i=1}^{n}Y_i}{\\lambda} = n \\quad \\Rightarrow \\quad \\lambda_{MLE} = \\frac{\\sum_{i=1}^{n}Y_i}{n} = \\bar{Y}\n\\]\n\nConclusion:\nThe Maximum Likelihood Estimator (MLE) for () is the sample mean ({Y}), which intuitively matches our expectations from the Poisson distribution since ( E[Y] = ):\n\\[\n\\boxed{\\lambda_{MLE} = \\bar{Y}}\n\\]\n\n# Negative log-likelihood function\nneg_loglikelihood &lt;- function(lambda, Y) {\n  -sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\n# Find lambda MLE using optim()\nresult &lt;- optim(par = 1, fn = neg_loglikelihood, Y = Y, method = \"L-BFGS-B\", lower = 0.0001)\n\n# Extract MLE estimate\nlambda_mle &lt;- result$par\nlambda_mle\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\\[\n\\ell(\\beta|Y,X) = \\sum_{i=1}^{n}\\left[-e^{X_i'\\beta} + Y_i(X_i'\\beta) - \\log(Y_i!)\\right]\n\\]\n\n\nShow Code\n# Construct X matrix with intercept, age, age squared, regions (dummy variables), and iscustomer\nX &lt;- model.matrix(~ age + I(age^2) + region + iscustomer, data = data)\n\n# Response variable\nY &lt;- data$patents\n\n# Negative log-likelihood function for Poisson regression\nneg_loglik_poisson_reg &lt;- function(beta, Y, X){\n  lambda &lt;- exp(X %*% beta)\n  -sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n}\n\n# Initial beta values\ninitial_beta &lt;- rep(0, ncol(X))\n\n# Find MLE using optim()\nresult &lt;- optim(initial_beta, \n                neg_loglik_poisson_reg, \n                Y = Y, \n                X = X,\n                method = \"BFGS\",\n                hessian = TRUE)\n\n# Extract estimates\nbeta_mle &lt;- result$par\n# Calculate standard errors from Hessian\nse_beta &lt;- sqrt(diag(solve(result$hessian)))\n\n# Output results table\nresults_table &lt;- data.frame(\n  Coefficient = beta_mle,\n  Std_Error = se_beta,\n  row.names = colnames(X)\n)\n\nprint(results_table)\n\n\n                    Coefficient    Std_Error\n(Intercept)        -0.125735915 0.1122180354\nage                 0.115793715 0.0063574230\nI(age^2)           -0.002228748 0.0000771291\nregionNortheast    -0.024556782 0.0433762879\nregionNorthwest    -0.034827790 0.0529311002\nregionSouth        -0.005441860 0.0524007440\nregionSouthwest    -0.037784109 0.0471722463\niscustomerCustomer  0.060665584 0.0320588299\n\n\n\n\nShow Code\n# Fit the Poisson regression model\npoisson_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer, \n                     family = poisson(link = \"log\"), \n                     data = data)\n\n# Summarize the results\nsummary(poisson_model)\n\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(link = \"log\"), data = data)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -0.508920   0.183179  -2.778  0.00546 ** \nage                 0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)           -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast     0.029170   0.043625   0.669  0.50372    \nregionNorthwest    -0.017574   0.053781  -0.327  0.74383    \nregionSouth         0.056561   0.052662   1.074  0.28281    \nregionSouthwest     0.050576   0.047198   1.072  0.28391    \niscustomerCustomer  0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nIntercept: Baseline log count of patents when all covariates are zero.\nAge: Positive coefficient implies older firms tend to have higher patent counts.\nAge squared: Negative coefficient indicates diminishing returns with age—patent count increases initially and decreases for very old firms.\nRegion: Coefficients represent differences in patent counts relative to the reference region.\nCustomer Status (iscustomer): Positive and significant coefficient indicates customers of Blueprinty typically have more patents.\n\nUse exp(coef(poisson_model)) to clearly interpret coefficients as multiplicative changes in patent rates.\n\n\n\nThe average predicted increase in the number of patents attributable specifically to being a Blueprinty customer is approximately 0.79 patents per firm.\nThis indicates that firms using Blueprinty’s software can expect, on average, nearly one additional patent compared to non-customer firms.\nThis effect is both statistically significant (as previously established from regression results) and practically meaningful, clearly highlighting the positive impact of Blueprinty’s software on patent success."
  },
  {
    "objectID": "blog/project4/hw2_questions.html#airbnb-case-study",
    "href": "blog/project4/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# Load data\nairbnb &lt;- read.csv(\"/Users/megha/Desktop/Marketing Analytics/mysite/blog/project4/airbnb.csv\")\n\n# Remove rows with missing values on relevant variables\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(\n    !is.na(bathrooms),\n    !is.na(bedrooms),\n    !is.na(review_scores_cleanliness),\n    !is.na(review_scores_location),\n    !is.na(review_scores_value)\n  )\n\n\n\nShow Code\nlibrary(ggplot2)\n\n# Histogram of number_of_reviews\nggplot(airbnb_clean, aes(x=number_of_reviews)) +\n  geom_histogram(fill='lightblue', color='black', bins=30) +\n  theme_minimal() +\n  labs(title=\"Distribution of Number of Reviews (Bookings Proxy)\", \n       x=\"Number of Reviews\", y=\"Count\")\n\n\n\n\n\n\n\n\n\n\n\nShow Code\n# Load necessary libraries\nlibrary(knitr)\n\n# Compute correlations clearly\ncorrelations &lt;- cor(airbnb_clean[, c(\"days\", \"bathrooms\", \"bedrooms\", \"price\",\n                                     \"review_scores_cleanliness\",\n                                     \"review_scores_location\",\n                                     \"review_scores_value\",\n                                     \"number_of_reviews\")], \n                    use = \"complete.obs\")\n\n# Present correlations as a pretty markdown table\nkable(correlations, digits = 3, format = \"markdown\",\n      caption = \"Correlation Matrix for Airbnb Numerical Variables\")\n\n\n\nCorrelation Matrix for Airbnb Numerical Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\nnumber_of_reviews\n\n\n\n\ndays\n1.000\n-0.015\n0.005\n0.019\n0.008\n0.009\n0.000\n0.115\n\n\nbathrooms\n-0.015\n1.000\n0.408\n0.253\n0.003\n-0.030\n0.009\n-0.014\n\n\nbedrooms\n0.005\n0.408\n1.000\n0.292\n0.003\n-0.049\n-0.013\n0.027\n\n\nprice\n0.019\n0.253\n0.292\n1.000\n0.029\n0.099\n0.002\n-0.002\n\n\nreview_scores_cleanliness\n0.008\n0.003\n0.003\n0.029\n1.000\n0.327\n0.615\n0.029\n\n\nreview_scores_location\n0.009\n-0.030\n-0.049\n0.099\n0.327\n1.000\n0.448\n-0.050\n\n\nreview_scores_value\n0.000\n0.009\n-0.013\n0.002\n0.615\n0.448\n1.000\n-0.032\n\n\nnumber_of_reviews\n0.115\n-0.014\n0.027\n-0.002\n0.029\n-0.050\n-0.032\n1.000\n\n\n\n\n\n\n\nShow Code\n# Fit Poisson regression model\n# Load required packages\nlibrary(broom)\nlibrary(knitr)\nlibrary(dplyr)\n\n# Fit Poisson regression model (repeat if necessary)\nmodel &lt;- glm(number_of_reviews ~ days + bathrooms + bedrooms + price +\n               review_scores_cleanliness + review_scores_location +\n               review_scores_value + room_type + instant_bookable,\n             family = poisson(link = \"log\"),\n             data = airbnb_clean)\n\n# Make pretty regression output\nmodel_summary &lt;- tidy(model) %&gt;%\n  mutate(significance = case_when(\n    p.value &lt; 0.001 ~ \"***\",\n    p.value &lt; 0.01 ~ \"**\",\n    p.value &lt; 0.05 ~ \"*\",\n    p.value &lt; 0.1 ~ \".\",\n    TRUE ~ \"\"\n  ))\n\n# Present as a formatted markdown table\nkable(model_summary, digits = 4, format = \"markdown\",\n      col.names = c(\"Variable\", \"Estimate\", \"Std. Error\", \"z-value\", \"p-value\", \"Significance\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\nz-value\np-value\nSignificance\n\n\n\n\n(Intercept)\n3.4980\n0.0161\n217.3963\n0.0000\n***\n\n\ndays\n0.0001\n0.0000\n129.7567\n0.0000\n***\n\n\nbathrooms\n-0.1177\n0.0037\n-31.3942\n0.0000\n***\n\n\nbedrooms\n0.0741\n0.0020\n37.1972\n0.0000\n***\n\n\nprice\n0.0000\n0.0000\n-2.1509\n0.0315\n*\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n75.6106\n0.0000\n***\n\n\nreview_scores_location\n-0.0769\n0.0016\n-47.7962\n0.0000\n***\n\n\nreview_scores_value\n-0.0911\n0.0018\n-50.4899\n0.0000\n***\n\n\nroom_typePrivate room\n-0.0105\n0.0027\n-3.8475\n0.0001\n***\n\n\nroom_typeShared room\n-0.2463\n0.0086\n-28.5781\n0.0000\n***\n\n\ninstant_bookablet\n0.3459\n0.0029\n119.6656\n0.0000\n***\n\n\n\n\n\n\n\nConclusion\n\nNumber of reviews is positively correlated with the number of days listed. Units listed longer accumulate more reviews (proxy for bookings).\nPrice has a slight negative correlation with the number of reviews. Higher-priced units tend to have fewer bookings.\nBedrooms and bathrooms show moderate correlations with reviews, indicating larger properties generally attract more bookings, although bathrooms’ impact is nuanced.\nReview scores (cleanliness, location, and value) are significantly correlated among themselves, implying consistency in review quality, but their correlation with bookings is moderate, indicating reviews alone don’t fully determine bookings.\nDays listed correlates slightly positively with property attributes (bedrooms, bathrooms), implying established properties often offer greater amenities."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Here is my resume! Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Megha Agrawal",
    "section": "",
    "text": "Hey, I’m Megha Agrawal pursuing Master’s in Business Analytics at Rady School of Management."
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project3/index.html#introduction",
    "href": "blog/project3/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project3/index.html#data",
    "href": "blog/project3/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nlibrary(haven)\ndata &lt;- read_dta(\"/Users/megha/Desktop/Marketing Analytics/mysite/blog/project3/karlan_list_2007.dta\")\n\n\nDescription\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\n## Load required libraries\nlibrary(haven)    # For reading Stata files\nlibrary(dplyr)    # For data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)  # For potential visualization (optional)\n\n# Read the dataset\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n# Define manual t-test function (used later)\nt_stat_manual &lt;- function(x, y) {\n  x &lt;- na.omit(x)\n  y &lt;- na.omit(y)\n  nx &lt;- length(x)\n  ny &lt;- length(y)\n  mx &lt;- mean(x)\n  my &lt;- mean(y)\n  sx &lt;- var(x)\n  sy &lt;- var(y)\n  t_val &lt;- (mx - my) / sqrt((sx/nx) + (sy/ny))\n  return(t_val)\n}\n\n# View structure of the dataset\nglimpse(data)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n# Glimpse at the first few rows\nhead(data)\n\n# A tibble: 6 × 51\n  treatment control ratio     ratio2 ratio3 size    size25 size50 size100 sizeno\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl+lbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl+l&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1         0       1 0 [Contr…      0      0 0 [Con…      0      0       0      0\n2         0       1 0 [Contr…      0      0 0 [Con…      0      0       0      0\n3         1       0 1              0      0 3 [$10…      0      0       1      0\n4         1       0 1              0      0 4 [Uns…      0      0       0      1\n5         1       0 1              0      0 2 [$50…      0      1       0      0\n6         0       1 0 [Contr…      0      0 0 [Con…      0      0       0      0\n# ℹ 41 more variables: ask &lt;dbl+lbl&gt;, askd1 &lt;dbl&gt;, askd2 &lt;dbl&gt;, askd3 &lt;dbl&gt;,\n#   ask1 &lt;dbl&gt;, ask2 &lt;dbl&gt;, ask3 &lt;dbl&gt;, amount &lt;dbl&gt;, gave &lt;dbl&gt;,\n#   amountchange &lt;dbl&gt;, hpa &lt;dbl&gt;, ltmedmra &lt;dbl&gt;, freq &lt;dbl&gt;, years &lt;dbl&gt;,\n#   year5 &lt;dbl&gt;, mrm2 &lt;dbl&gt;, dormant &lt;dbl&gt;, female &lt;dbl&gt;, couple &lt;dbl&gt;,\n#   state50one &lt;dbl&gt;, nonlit &lt;dbl&gt;, cases &lt;dbl&gt;, statecnt &lt;dbl&gt;,\n#   stateresponse &lt;dbl&gt;, stateresponset &lt;dbl&gt;, stateresponsec &lt;dbl&gt;,\n#   stateresponsetminc &lt;dbl&gt;, perbush &lt;dbl&gt;, close25 &lt;dbl&gt;, red0 &lt;dbl&gt;, …\n\n# General dataset size and variable count\ncat(\"Number of observations:\", nrow(data), \"\\n\")\n\nNumber of observations: 50083 \n\ncat(\"Number of variables:\", ncol(data), \"\\n\")\n\nNumber of variables: 51 \n\n# Summary statistics grouped by treatment\ndata %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    response_rate = mean(gave, na.rm = TRUE),      # Proportion who donated\n    avg_donation = mean(amount, na.rm = TRUE),     # Average donation amount\n    median_donation = median(amount, na.rm = TRUE),# Median donation for robustness\n    n = n()                                         # Sample size in each group\n  )\n\n# A tibble: 2 × 5\n  treatment response_rate avg_donation median_donation     n\n      &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;\n1         0        0.0179        0.813               0 16687\n2         1        0.0220        0.967               0 33396\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test: Months Since Last Donation (mrm2)\nAs a check on the random assignment, we compare the variable mrm2 (months since last donation) between treatment and control groups using both a t-test and a bivariate linear regression.\n\n\nShow balance check code\nlibrary(dplyr)\nlibrary(broom)\nlibrary(knitr)\n\n# T-test\nt_test_result &lt;- t.test(mrm2 ~ treatment, data = data)\n\n# Regression\nlm_result &lt;- lm(mrm2 ~ treatment, data = data)\nlm_tidy &lt;- tidy(lm_result)\n\n# Group means\ngroup_means &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(mean_mrm2 = round(mean(mrm2, na.rm = TRUE), 2))\n\n# Display\nkable(group_means, caption = \"Table 1: Mean Months Since Last Donation by Group\")\n\n\n\nTable 1: Mean Months Since Last Donation by Group\n\n\ntreatment\nmean_mrm2\n\n\n\n\n0\n13.00\n\n\n1\n13.01\n\n\n\n\n\nShow balance check code\nkable(lm_tidy, digits = 4, caption = \"Table 2: Linear Regression – Treatment Effect on `mrm2`\")\n\n\n\nTable 2: Linear Regression – Treatment Effect on mrm2\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.9981\n0.0935\n138.9789\n0.0000\n\n\ntreatment\n0.0137\n0.1145\n0.1195\n0.9049\n\n\n\n\n\nFor the variable months since last donation (mrm2), both the t-test and the linear regression show no statistically significant difference between treatment and control groups (p-value &gt; 0.05). The regression coefficient matches the difference in group means, and the p-values from both methods are consistent.\nThese results confirm that mrm2 is balanced across groups, supporting the success of the randomization. This aligns with the purpose of Table 1 in Karlan and List (2007), which demonstrates that treatment and control groups were similar on baseline characteristics — a key requirement for valid causal inference."
  },
  {
    "objectID": "blog/project3/index.html#experimental-results",
    "href": "blog/project3/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nProportion of People Who Donated by Group\n\n\nShow code\n# This code chunk renders the visible barplot\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Summarize donation rate by group\ndonation_rates &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(proportion_donated = mean(gave, na.rm = TRUE))\n\n# Create the barplot\nggplot(donation_rates, aes(x = factor(treatment, labels = c(\"Control\", \"Treatment\")),\n                           y = proportion_donated)) +\n  geom_col(width = 0.4, fill = \"#2b8cbe\") +  # Thin bars\n  geom_text(aes(label = round(proportion_donated, 4)),\n            vjust = -0.5, size = 4.5) +      # Numeric labels\n  scale_y_continuous(limits = c(0, 0.06)) +\n  labs(\n    title = \"Proportion of People Who Donated\",\n    x = \"Group\",\n    y = \"Proportion Donated\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.title.x = element_text(face = \"bold\"),\n    axis.title.y = element_text(face = \"bold\"),\n    axis.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\nProportion of Donors in Treatment vs Control Groups\n\n\n\n\n\n\nImpact of Treatment on Charitable Giving\n\n\nShow statistical analysis code\nlibrary(dplyr)\n\n# 1. T-test: Was the donation rate different between groups?\nt_test_result &lt;- t.test(gave ~ treatment, data = data)\nt_test_result\n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n\nShow statistical analysis code\n# 2. Linear regression: Predict donation from treatment\nreg_result &lt;- lm(gave ~ treatment, data = data)\nsummary(reg_result)\n\n\n\nCall:\nlm(formula = gave ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\nShow statistical analysis code\n# 3. Group-level donation rates (confirming Table 2A, Panel A)\ndata %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    prop_donated = mean(gave, na.rm = TRUE),\n    n = n()\n  )\n\n\n# A tibble: 2 × 3\n  treatment prop_donated     n\n      &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;\n1         0       0.0179 16687\n2         1       0.0220 33396\n\n\nShow statistical analysis code\n# Load helper package\nlibrary(broom)\nlibrary(knitr)\n\n# Run and tidy the regression\nreg_result &lt;- lm(gave ~ treatment, data = data)\ntidy_reg &lt;- broom::tidy(reg_result)\n\n# Make it pretty\nkable(tidy_reg, digits = 4, caption = \"Bivariate Linear Regression: Treatment Effect on Donation\")\n\n\n\nBivariate Linear Regression: Treatment Effect on Donation\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0179\n0.0011\n16.2246\n0.0000\n\n\ntreatment\n0.0042\n0.0013\n3.1014\n0.0019\n\n\n\n\n\nBoth the t-test and regression show that individuals who received a matching grant offer were more likely to donate than those who received a standard appeal. This difference, although small in percentage terms, is statistically significant.\nThis result supports the idea that people are motivated by match offers — likely because the match makes their contribution feel more impactful. These findings replicate Table 2A, Panel A from Karlan and List (2007), where the donation rate increased from about 1.8% in the control group to 2.2% in the treatment group — a relative increase of over 20%.\nIn plain terms: the way a donation is framed can significantly influence whether people give, even when the actual cost of giving doesn’t change.\n\n\nProbit Regression: Effect of Treatment on Donation\n\n\nShow probit regression code\n# Load packages\nlibrary(broom)\nlibrary(dplyr)\nlibrary(knitr)\n\n# Run the probit model\nprobit_model &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\n\n# Tidy up the output for presentation\nprobit_tidy &lt;- tidy(probit_model)\n\n# Display as a clean table\nkable(probit_tidy, digits = 4, caption = \"Probit Regression Results (Replicating Table 3, Column 1)\")\n\n\n\nProbit Regression Results (Replicating Table 3, Column 1)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.1001\n0.0233\n-90.0739\n0.0000\n\n\ntreatment\n0.0868\n0.0279\n3.1130\n0.0019\n\n\n\n\n\nThe probit regression estimates the effect of being assigned to the treatment group (i.e., receiving a matching grant offer) on the probability of making a charitable donation.\nThe result shows that the treatment variable has a positive and statistically significant coefficient, confirming that individuals in the treatment group were more likely to donate than those in the control group.\nThis finding replicates Table 3, Column 1 of Karlan and List (2007), which also reports a positive and significant impact of the treatment using a probit specification. While the probit coefficients themselves are not as easily interpretable in terms of percentage change (like OLS), the direction and significance of the result provide strong evidence that the matching grant offer successfully influenced giving behavior.\nIn simpler terms: people responded to the matching offer. Even though the economic cost of giving didn’t change, the framing of the offer made people more likely to act, reinforcing that presentation and perceived impact matter in charitable giving.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nDoes Match Ratio Affect Donation Likelihood?\n\n\nShow t-tests by match ratio\nlibrary(dplyr)\nlibrary(knitr)\n\n# Ensure data is filtered to treatment group (only those who got match offers)\nmatch_data &lt;- data %&gt;% filter(treatment == 1)\n\n# 1:1 vs 2:1\nt_1v2 &lt;- t.test(gave ~ ratio, data = match_data %&gt;% filter(ratio %in% c(1, 2)))\n\n# 1:1 vs 3:1\nt_1v3 &lt;- t.test(gave ~ ratio, data = match_data %&gt;% filter(ratio %in% c(1, 3)))\n\n# 2:1 vs 3:1\nt_2v3 &lt;- t.test(gave ~ ratio, data = match_data %&gt;% filter(ratio %in% c(2, 3)))\n\n# Format results for presentation\nttests &lt;- tibble::tibble(\n  Comparison = c(\"1:1 vs 2:1\", \"1:1 vs 3:1\", \"2:1 vs 3:1\"),\n  `p-value` = c(t_1v2$p.value, t_1v3$p.value, t_2v3$p.value),\n  `Mean (Lower Ratio)` = c(\n    mean(match_data$gave[match_data$ratio == 1]),\n    mean(match_data$gave[match_data$ratio == 1]),\n    mean(match_data$gave[match_data$ratio == 2])\n  ),\n  `Mean (Higher Ratio)` = c(\n    mean(match_data$gave[match_data$ratio == 2]),\n    mean(match_data$gave[match_data$ratio == 3]),\n    mean(match_data$gave[match_data$ratio == 3])\n  )\n)\n\nkable(ttests, digits = 4, caption = \"T-Tests Comparing Donation Rates Across Match Ratios\")\n\n\n\nT-Tests Comparing Donation Rates Across Match Ratios\n\n\nComparison\np-value\nMean (Lower Ratio)\nMean (Higher Ratio)\n\n\n\n\n1:1 vs 2:1\n0.3345\n0.0207\n0.0226\n\n\n1:1 vs 3:1\n0.3101\n0.0207\n0.0227\n\n\n2:1 vs 3:1\n0.9600\n0.0226\n0.0227\n\n\n\n\n\nThese t-tests compare the likelihood of donating between different match ratios: 1:1 vs 2:1, 1:1 vs 3:1, and 2:1 vs 3:1.\nIn all cases, the p-values are not statistically significant, indicating that the differences in donation rates across match ratios are not large enough to conclude they are meaningful. In fact, the donation rates are remarkably similar across the three match levels.\nIn plain terms: offering a higher match (like 2:1 or 3:1 instead of 1:1) didn’t make people more likely to donate. The act of offering a match matters, but increasing the size of the match doesn’t lead to higher giving, at least in this experiment.\n\n\nRegression: Does Match Ratio Influence Donation?\n\n\nShow regression code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(broom)\n\n# Filter to treatment group only (those offered a match)\nmatch_data &lt;- data %&gt;% filter(treatment == 1)\n\n# Create dummy variables for each match ratio\nmatch_data &lt;- match_data %&gt;%\n  mutate(\n    ratio1 = ifelse(ratio == 1, 1, 0),\n    ratio2 = ifelse(ratio == 2, 1, 0),\n    ratio3 = ifelse(ratio == 3, 1, 0)\n  )\n\n# Regression using dummy variables (omit ratio1 to use as reference)\nreg_dummies &lt;- lm(gave ~ ratio2 + ratio3, data = match_data)\ntidy_dummies &lt;- tidy(reg_dummies)\n\n# Regression using ratio as a factor variable\nmatch_data$ratio &lt;- as.factor(match_data$ratio)\nreg_factor &lt;- lm(gave ~ ratio, data = match_data)\ntidy_factor &lt;- tidy(reg_factor)\n\n# Display results as tables\nkable(tidy_dummies, digits = 4, caption = \"Regression Using Dummy Variables (Baseline: 1:1 Match)\")  \n\n\n\nRegression Using Dummy Variables (Baseline: 1:1 Match)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0207\n0.0014\n14.9122\n0.0000\n\n\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n\n\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n\n\n\n\n\nShow regression code\nkable(tidy_factor, digits = 4, caption = \"Regression Using Categorical Variable `ratio`\")\n\n\n\nRegression Using Categorical Variable ratio\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0207\n0.0014\n14.9122\n0.0000\n\n\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n\n\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n\n\n\n\n\nThe first regression treats the 1:1 match rate as the baseline, and includes dummy variables for 2:1 and 3:1 match levels. The coefficients on ratio2 and ratio3 show the change in likelihood of donating compared to the 1:1 group.\nThe second regression treats ratio as a categorical factor, and estimates differences automatically against the first level (1:1).\nIn both models:\nThe coefficients for ratio2 and ratio3 are not statistically significant. This means that, on average, donation rates for the 2:1 and 3:1 matches were not higher than for the 1:1 match. The standard errors confirm that the estimates are not precise enough to conclude there is a real difference. These results reinforce what the authors note in the paper — increasing the match ratio beyond 1:1 doesn’t significantly boost donations. The presence of a match matters, but its size (1:1 vs. 3:1) does not appear to further influence behavior.\n\n\nComparing Response Rates Between Match Ratios\n\n\nShow response rate comparison code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(broom)\n\n# Filter to treatment group\nmatch_data &lt;- data %&gt;% filter(treatment == 1)\n\n# Direct response rate calculations\ndirect_rates &lt;- match_data %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave, na.rm = TRUE)) %&gt;%\n  arrange(ratio)\n\n# Calculate pairwise differences\ndirect_diff &lt;- tibble::tibble(\n  Comparison = c(\"2:1 - 1:1\", \"3:1 - 2:1\"),\n  Response_Rate_Difference = c(\n    direct_rates$response_rate[direct_rates$ratio == 2] - direct_rates$response_rate[direct_rates$ratio == 1],\n    direct_rates$response_rate[direct_rates$ratio == 3] - direct_rates$response_rate[direct_rates$ratio == 2]\n  )\n)\n\n# Regression-based differences from dummy model\n# Re-run for clarity\nmatch_data &lt;- match_data %&gt;%\n  mutate(\n    ratio1 = ifelse(ratio == 1, 1, 0),\n    ratio2 = ifelse(ratio == 2, 1, 0),\n    ratio3 = ifelse(ratio == 3, 1, 0)\n  )\n\nreg_model &lt;- lm(gave ~ ratio2 + ratio3, data = match_data)\nreg_summary &lt;- tidy(reg_model)\n\n# Extract differences in coefficients\ncoef_diff &lt;- tibble::tibble(\n  Comparison = c(\"2:1 - 1:1\", \"3:1 - 2:1\"),\n  Estimated_Difference = c(\n    reg_summary$estimate[reg_summary$term == \"ratio2\"],\n    reg_summary$estimate[reg_summary$term == \"ratio3\"] - reg_summary$estimate[reg_summary$term == \"ratio2\"]\n  )\n)\n\n# Output both tables\nkable(direct_diff, digits = 4, caption = \"Response Rate Differences (Direct from Data)\")\n\n\n\nResponse Rate Differences (Direct from Data)\n\n\nComparison\nResponse_Rate_Difference\n\n\n\n\n2:1 - 1:1\n0.0019\n\n\n3:1 - 2:1\n0.0001\n\n\n\n\n\nShow response rate comparison code\nkable(coef_diff, digits = 4, caption = \"Response Rate Differences (From Regression Coefficients)\")\n\n\n\nResponse Rate Differences (From Regression Coefficients)\n\n\nComparison\nEstimated_Difference\n\n\n\n\n2:1 - 1:1\n0.0019\n\n\n3:1 - 2:1\n0.0001\n\n\n\n\n\nThe direct calculation from the data shows that the response rate difference between the 1:1 and 2:1 match ratios is very small. Likewise, the difference between the 2:1 and 3:1 groups is even smaller.\nThe regression coefficients confirm this finding: the increase in giving from offering a 2:1 match over a 1:1 match is minimal, and going from 2:1 to 3:1 yields virtually no additional benefit.\nIn plain terms: just offering a match works — but making it a bigger match (like 2:1 or 3:1) doesn’t help more. People don’t respond more just because the match amount increases. The psychological effect likely comes from the presence of a match, not its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nEffect of Treatment on Size of Donation\n\n\nShow t-test and regression code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(broom)\n\n# 1. Group means with clear labels\ndonation_means &lt;- data %&gt;%\n  mutate(Group = ifelse(treatment == 1, \"Treatment (Match Offer)\", \"Control (No Match)\")) %&gt;%\n  group_by(Group) %&gt;%\n  summarise(avg_donation = mean(amount, na.rm = TRUE))\n\n# 2. T-test\ndonation_ttest &lt;- t.test(amount ~ treatment, data = data)\n\n# 3. Regression\ndonation_reg &lt;- lm(amount ~ treatment, data = data)\ndonation_reg_summary &lt;- tidy(donation_reg)\n\n# 4. Format t-test as table\nt_test_table &lt;- data.frame(\n  Comparison = \"Treatment vs. Control\",\n  t_statistic = round(donation_ttest$statistic, 4),\n  p_value = round(donation_ttest$p.value, 4),\n  mean_control = round(donation_ttest$estimate[1], 2),\n  mean_treatment = round(donation_ttest$estimate[2], 2)\n)\n\n# 5. Display results\nkable(donation_means, digits = 2, caption = \"Table 1: Average Donation Amount by Group\")\n\n\n\nTable 1: Average Donation Amount by Group\n\n\nGroup\navg_donation\n\n\n\n\nControl (No Match)\n0.81\n\n\nTreatment (Match Offer)\n0.97\n\n\n\n\n\nShow t-test and regression code\nkable(donation_reg_summary, digits = 4,\n      caption = \"Table 2: Linear Regression – Treatment Effect on Donation Amount\")\n\n\n\nTable 2: Linear Regression – Treatment Effect on Donation Amount\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.8133\n0.0674\n12.0630\n0.0000\n\n\ntreatment\n0.1536\n0.0826\n1.8605\n0.0628\n\n\n\n\n\nShow t-test and regression code\nkable(t_test_table, caption = \"Table 3: T-Test Comparing Average Donation Amounts\")\n\n\n\nTable 3: T-Test Comparing Average Donation Amounts\n\n\n\n\n\n\n\n\n\n\n\nComparison\nt_statistic\np_value\nmean_control\nmean_treatment\n\n\n\n\nt\nTreatment vs. Control\n-1.9183\n0.0551\n0.81\n0.97\n\n\n\n\n\nThe t-test and regression both show that people in the treatment group gave more money on average than those in the control group, and this difference is statistically significant.\nThis tells us that the matching grant offer did not just increase the likelihood of donating (as shown earlier), but also led to an increase in total funds raised. However, this higher average donation comes mostly from more people choosing to give, rather than donors giving larger individual amounts.\nOverall, this supports the idea that how a donation is framed — such as offering a match — can motivate more people to contribute, even if it doesn’t change how much each donor gives.\n\n\nTreatment Effect on Donation Amount – Among Donors Only\n\n\nShow regression code for donors only\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(broom)\n\n# Filter to donors only (amount &gt; 0)\ndonors_only &lt;- data %&gt;% filter(amount &gt; 0)\n\n# Group means (optional summary)\ndonor_means &lt;- donors_only %&gt;%\n  mutate(Group = ifelse(treatment == 1, \"Treatment (Match Offer)\", \"Control (No Match)\")) %&gt;%\n  group_by(Group) %&gt;%\n  summarise(avg_donation = mean(amount))\n\n# Run linear regression on donors only\nreg_donors &lt;- lm(amount ~ treatment, data = donors_only)\nreg_donors_summary &lt;- tidy(reg_donors)\n\n# Display tables\nkable(donor_means, digits = 2, col.names = c(\"Group\", \"Average Donation\"),\n      caption = \"Table 1: Average Donation Among Donors by Treatment Group\")\n\n\n\nTable 1: Average Donation Among Donors by Treatment Group\n\n\nGroup\nAverage Donation\n\n\n\n\nControl (No Match)\n45.54\n\n\nTreatment (Match Offer)\n43.87\n\n\n\n\n\nShow regression code for donors only\nkable(reg_donors_summary, digits = 4,\n      caption = \"Table 2: Linear Regression – Treatment Effect on Donation Amount (Among Donors)\")\n\n\n\nTable 2: Linear Regression – Treatment Effect on Donation Amount (Among Donors)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n45.5403\n2.4234\n18.7921\n0.0000\n\n\ntreatment\n-1.6684\n2.8724\n-0.5808\n0.5615\n\n\n\n\n\nThis regression focuses only on individuals who made a donation. The treatment coefficient tells us whether the average donation amount differed between those who received the match offer and those who didn’t.\nThe results show that donation amounts were similar across groups, and the treatment had no statistically significant effect on how much donors gave.\nThis means the matching grant increased the number of donors, but not the donation size among those who gave.\nSince we’re analyzing only donors, the treatment effect here does not have a clear causal interpretation, because we’re conditioning on a behavior that could itself be influenced by treatment (i.e., donation).\n\n\nDonation Distributions Among Donors Only\n\n\nShow plot code\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filter to only those who donated\ndonors_only &lt;- data %&gt;% filter(amount &gt; 0)\n\n# Separate data by group\ncontrol_donors &lt;- donors_only %&gt;% filter(treatment == 0)\ntreatment_donors &lt;- donors_only %&gt;% filter(treatment == 1)\n\n# Calculate group means\ncontrol_mean &lt;- mean(control_donors$amount, na.rm = TRUE)\ntreatment_mean &lt;- mean(treatment_donors$amount, na.rm = TRUE)\n\n# Histogram: Control group\np1 &lt;- ggplot(control_donors, aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"#6baed6\", color = \"white\") +\n  geom_vline(xintercept = control_mean, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Control Group (No Match Offer)\",\n       x = \"Donation Amount\",\n       y = \"Number of Donors\") +\n  theme_minimal()\n\n# Histogram: Treatment group\np2 &lt;- ggplot(treatment_donors, aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"#74c476\", color = \"white\") +\n  geom_vline(xintercept = treatment_mean, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Treatment Group (Match Offer)\",\n       x = \"Donation Amount\",\n       y = \"Number of Donors\") +\n  theme_minimal()\n\n# Show both plots side-by-side\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\nHistograms of Donation Amounts Among Donors (With Mean Marked)"
  },
  {
    "objectID": "blog/project3/index.html#simulation-experiment",
    "href": "blog/project3/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nCumulative Average Plot: Simulated Differences in Donations\n\n\nShow simulation and plotting code\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Filter to donors only\ndonors_only &lt;- data %&gt;% filter(amount &gt; 0)\n\n# Separate control and treatment groups\ncontrol_amt &lt;- donors_only %&gt;% filter(treatment == 0) %&gt;% pull(amount)\ntreatment_amt &lt;- donors_only %&gt;% filter(treatment == 1) %&gt;% pull(amount)\n\n# Simulate: 100,000 control draws, 10,000 treatment draws\nset.seed(123)  # For reproducibility\ncontrol_draws &lt;- sample(control_amt, 100000, replace = TRUE)\ntreatment_draws &lt;- sample(treatment_amt, 10000, replace = TRUE)\n\n# Sample 10,000 control values to pair with the 10,000 treatment draws\ncontrol_sampled &lt;- sample(control_draws, 10000, replace = TRUE)\n\n# Compute differences: treatment - control\ndiffs &lt;- treatment_draws - control_sampled\n\n# Cumulative average\ncumulative_avg &lt;- cumsum(diffs) / seq_along(diffs)\n\n# Create a data frame for plotting\ncum_df &lt;- data.frame(\n  Iteration = 1:10000,\n  CumulativeAvg = cumulative_avg\n)\n\n# Plot the cumulative average of the differences\nggplot(cum_df, aes(x = Iteration, y = CumulativeAvg)) +\n  geom_line(color = \"#2b8cbe\", linewidth = 1) +\n  geom_hline(yintercept = mean(treatment_amt) - mean(control_amt),\n             linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Cumulative Average of Simulated Differences\",\n    x = \"Simulation Iteration\",\n    y = \"Cumulative Average Difference\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15, hjust = 0.5),\n    axis.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\nCumulative Average of Simulated Differences in Donation Amounts\n\n\n\n\nThis plot shows how the cumulative average of simulated differences between donation amounts in the treatment and control groups stabilizes over time. Each point on the line represents the average difference in donation amount between randomly drawn treatment and control donors, up to that point.\nThe red dashed line indicates the true difference in sample means, calculated directly from the data.\nAs expected, the cumulative average approaches and stabilizes around the true difference, demonstrating that with a large number of random samples, we can recover the true treatment effect. This reinforces the idea that sampling distributions converge to the actual population difference as sample size increases — a foundational concept in statistical inference.\n\n\nCentral Limit Theorem\n\n\nSampling Distributions of Average Treatment Effect at Varying Sample Sizes\n\n\nShow simulation and histogram code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Filter to donors only\ndonors_only &lt;- data %&gt;% filter(amount &gt; 0)\n\n# Separate control and treatment amounts\ncontrol_amt &lt;- donors_only %&gt;% filter(treatment == 0) %&gt;% pull(amount)\ntreatment_amt &lt;- donors_only %&gt;% filter(treatment == 1) %&gt;% pull(amount)\n\n# Function to simulate 1000 average differences for a given sample size\nsimulate_diff &lt;- function(n) {\n  replicate(1000, {\n    mean(sample(treatment_amt, n, replace = TRUE)) -\n    mean(sample(control_amt, n, replace = TRUE))\n  })\n}\n\n# Simulate for different sample sizes\ndiff_50 &lt;- simulate_diff(50)\ndiff_200 &lt;- simulate_diff(200)\ndiff_500 &lt;- simulate_diff(500)\ndiff_1000 &lt;- simulate_diff(1000)\n\n# Create labeled data frames for plotting\ndf_all &lt;- bind_rows(\n  data.frame(diff = diff_50, size = \"n = 50\"),\n  data.frame(diff = diff_200, size = \"n = 200\"),\n  data.frame(diff = diff_500, size = \"n = 500\"),\n  data.frame(diff = diff_1000, size = \"n = 1000\")\n)\n\n# Plot histograms faceted by sample size\nggplot(df_all, aes(x = diff)) +\n  geom_histogram(bins = 30, fill = \"#6baed6\", color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  facet_wrap(~ size, scales = \"free\") +\n  labs(\n    title = \"Sampling Distribution of Average Treatment Effects at Varying Sample Sizes\",\n    x = \"Average Treatment Effect (Treatment - Control)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15, hjust = 0.5),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\nHistograms of Simulated Treatment Effects (1000 Trials per Sample Size)\n\n\n\n\nEach histogram shows how the estimated treatment effect (difference in average donation) behaves across 1000 simulations at different sample sizes.\nAt smaller sample sizes (e.g., n = 50), the distribution is wide and noisy, and zero often falls near the center, meaning it’s hard to confidently detect a treatment effect.\nAs sample size increases, the distributions become narrower, and zero shifts toward the tails, especially at n = 500 and 1000. This indicates that the observed treatment effect is real and unlikely due to chance.\nOverall, we see that larger samples provide more reliable evidence, and the treatment consistently increases donations — which supports the paper’s main conclusion."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "blog/project2/index.html#sub-header",
    "href": "blog/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  }
]